<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2019-04-12
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 3.3.0-SNAPSHOT &#x2013; Output: ,  and </title>
    <style type="text/css" media="all">
      @import url("../css/maven-base.css");
      @import url("../css/maven-theme.css");
      @import url("../css/site.css");
    </style>
    <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20190412" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                   <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
        &gt;
                  <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
        &gt;
                  <a href="../../index.html">Apache Hadoop Project Dist POM</a>
        &gt;
                  <a href="../index.html">Apache Hadoop 3.3.0-SNAPSHOT</a>
        &gt;
        Output: ,  and 
        </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://gitbox.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                   &nbsp;| Last Published: 2019-04-12
              &nbsp;| Version: 3.3.0-SNAPSHOT
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility Specification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DownstreamDev.html">Downstream Developer's Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html">Admin Compatibility Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FairCallQueue.html">Fair Call Queue</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">Disk Balancer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html">Router Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html">Provided Storage</a>
            </li>
          </ul>
                       <h5>Ozone</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/OzoneGettingStarted.html">Getting Started</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/OzoneOverview.html">Ozone Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/OzoneCommandShell.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/OzoneRest.html">Ozone Rest API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/OzoneMetrics.html">Ozone Metrics</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceModel.html">Resource Model</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeAttributes.html">Node Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/registry/index.html">Registry</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/UsingGpus.html">Using GPU</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/UsingFPGA.html">Using FPGA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/PlacementConstraints.html">Placement Constraints</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>YARN Service</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/Overview.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html">QuickStart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/Concepts.html">Concepts</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/YarnServiceAPI.html">Yarn Service API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/ServiceDiscovery.html">Service Discovery</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/yarn-service/SystemServices.html">System Services</a>
            </li>
          </ul>
                       <h5>Submarine</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/Index.html">Index</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/QuickStart.html">QuickStart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/Examples.html">Examples</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-aliyun/tools/hadoop-aliyun/index.html">Aliyun OSS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-openstack/index.html">OpenStack Swift</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/release/index.html">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml">hdfs-rbf-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<ul>
<li><a href="#Output_Stream_Model">Output Stream Model</a>
<ul>
<li><a href="#State_of_Stream_and_filesystem_after_Filesystem.create.28.29">State of Stream and filesystem after Filesystem.create()</a></li>
<li><a href="#State_of_Stream_and_filesystem_after_Filesystem.append.28.29">State of Stream and filesystem after Filesystem.append()</a>
<ul>
<li><a href="#Persisting_data">Persisting data</a></li></ul></li></ul></li>
<li><a href="#Class_java.io.OutputStream">Class java.io.OutputStream</a>
<ul>
<li><a href="#write.28Stream.2C_data.29">write(Stream, data)</a>
<ul>
<li><a href="#Preconditions">Preconditions</a></li>
<li><a href="#Postconditions">Postconditions</a></li></ul></li>
<li><a href="#write.28Stream.2C_byte.5B.5D_data.2C_int_offset.2C_int_len.29">write(Stream, byte[] data, int offset, int len)</a>
<ul>
<li><a href="#Preconditions">Preconditions</a></li>
<li><a href="#Postconditions">Postconditions</a></li></ul></li>
<li><a href="#write.28byte.5B.5D_data.29">write(byte[] data)</a></li>
<li><a href="#flush.28.29">flush()</a>
<ul>
<li><a href="#Preconditions">Preconditions</a></li>
<li><a href="#Postconditions">Postconditions</a></li></ul></li>
<li><a href="#close.28.29">close()</a></li></ul></li>
<li><a href="#Interface_StreamCapabilities">Interface StreamCapabilities</a></li>
<li><a href="#org.apache.hadoop.fs.Syncable">org.apache.hadoop.fs.Syncable</a>
<ul>
<li><a href="#Syncable.hflush.28.29">Syncable.hflush()</a>
<ul>
<li><a href="#Preconditions">Preconditions</a></li>
<li><a href="#Postconditions">Postconditions</a></li></ul></li>
<li><a href="#Syncable.hsync.28.29">Syncable.hsync()</a>
<ul>
<li><a href="#Preconditions">Preconditions</a></li>
<li><a href="#Postconditions">Postconditions</a></li></ul></li></ul></li>
<li><a href="#interface_CanSetDropBehind">interface CanSetDropBehind</a></li>
<li><a href="#Durability.2C_Concurrency.2C_Consistency_and_Visibility_of_stream_output.">Durability, Concurrency, Consistency and Visibility of stream output.</a></li>
<li><a href="#Durability">Durability</a></li>
<li><a href="#Concurrency">Concurrency</a></li>
<li><a href="#Consistency_and_Visibility">Consistency and Visibility</a></li>
<li><a href="#Issues_with_the_Hadoop_Output_Stream_model.">Issues with the Hadoop Output Stream model.</a>
<ul>
<li><a href="#HDFS">HDFS</a></li>
<li><a href="#Local_Filesystem.2C_file:">Local Filesystem, file:</a></li>
<li><a href="#Checksummed_output_streams">Checksummed output streams</a></li>
<li><a href="#Object_Stores">Object Stores</a>
<ul>
<li><a href="#Visibility_of_newly_created_files">Visibility of newly created files</a></li>
<li><a href="#Visibility_of_the_output_of_a_stream_after_close.28.29">Visibility of the output of a stream after close()</a></li>
<li><a href="#Other_issues">Other issues</a></li></ul></li></ul></li></ul>

<h1>Output: <tt>OutputStream</tt>, <tt>Syncable</tt> and <tt>StreamCapabilities</tt></h1>
<p>With the exception of <tt>FileSystem.copyFromLocalFile()</tt>, all API operations which write data to a filesystem in Hadoop do so through the Java &#x201c;OutputStreams&#x201d; API. More specifically, they do so through <tt>OutputStream</tt> subclasses obtained through calls to <tt>FileSystem.create()</tt>, <tt>FileSystem.append()</tt>, or <tt>FSDataOutputStreamBuilder.build()</tt>.</p>
<p>These all return instances of <tt>FSDataOutputStream</tt>, through which data can be written through various <tt>write()</tt> methods. After a stream&#x2019;s <tt>close()</tt> method is called, all data written to the stream MUST BE persisted to the fileysystem and visible to oll other clients attempting to read data from that path via <tt>FileSystem.open()</tt>.</p>
<p>As well as operations to write the data, Hadoop&#x2019;s Output Streams provide methods to flush buffered data back to the filesystem, so as to ensure that the data is reliably persisted and/or visible to other callers. This is done via the <tt>Syncable</tt> interface. It was originally intended that the presence of this interface could be interpreted as a guarantee that the stream supported it&#x2019;s methods, but this has proven impossible to guarantee as the static nature of the interface is incompatible with filesystems whose syncability semantics may vary on a store/path basis. As an example, erasure coded files in HDFS have A new interface, <tt>StreamCapabilities</tt> has been implemented to allow callers to probe the exact capabilities of a stream, even transitively through a chain of streams.</p>
<ul>

<li>HDFS&#x2019;s primary stream implementation is <tt>org.apache.hadoop.hdfs.DFSOutputStream</tt>.</li>
<li>The subclass <tt>org.apache.hadoop.hdfs.DFSStripedOutputStream</tt> supports erasure coding: it removes the <tt>Syncable</tt> behaviors from the base class.</li>
<li>The output streams <tt>org.apache.hadoop.fs.FSOutputSummer</tt> and <tt>org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSOutputSummer</tt> contain the underlying checksummed output stream used by both HDFS and the &#x201c;file&#x201d; filesystems.</li>
</ul>
<div class="section">
<h2><a name="Output_Stream_Model"></a>Output Stream Model</h2>
<p>For this specification, an output stream can be viewed as a list of bytes stored in in the client</p>

<div>
<div>
<pre class="source">buffer: List[byte]`
</pre></div></div>

<p>A flag, <tt>open</tt> tracks whether the stream is open: after the stream is closed no more data may be written to it:</p>

<div>
<div>
<pre class="source">open: bool
buffer: List[byte]
</pre></div></div>

<p>The destination path of the stream, <tt>path</tt> can be tracked to form a triple <tt>Path, open, buffer</tt></p>

<div>
<div>
<pre class="source">Stream = (path, open, buffer)
</pre></div></div>

<p>(Immediately) after <tt>Syncable</tt> operations which flush data to the filesystem, the data at the stream&#x2019;s destination path must match that of <tt>buffer</tt>. That is, the following condition holds:</p>

<div>
<div>
<pre class="source">FS'.Files(path) == buffer
</pre></div></div>

<p>Any client reading the data at the path will see the new data. The two sync operations, <tt>hflush()</tt> and <tt>hsync()</tt> differ in their durability guarantees, not visibility of data.</p>
<div class="section">
<h3><a name="State_of_Stream_and_filesystem_after_Filesystem.create.28.29"></a>State of Stream and filesystem after <tt>Filesystem.create()</tt></h3>
<p>The output stream returned by a <tt>FileSystem.create(path)</tt> call contains no data:</p>

<div>
<div>
<pre class="source">Stream' = (path, true, [])
</pre></div></div>

<p>The filesystem <tt>FS'</tt> must contain a 0-byte file at the path:</p>

<div>
<div>
<pre class="source">data(FS', path) == []
</pre></div></div>

<p>Accordingly, the the initial state of <tt>Stream'.buffer</tt> is implicitly consistent with the data at the filesystem.</p>
<p><i>Object Stores</i>: the 0-byte empty file may not exist in the filesystem.</p></div>
<div class="section">
<h3><a name="State_of_Stream_and_filesystem_after_Filesystem.append.28.29"></a>State of Stream and filesystem after <tt>Filesystem.append()</tt></h3>
<p>The output stream returned from a call of <tt>FileSystem.append(path, buffersize, progress)</tt>, can be modelled as a stream whose <tt>buffer</tt> is intialized to that of the original file:</p>

<div>
<div>
<pre class="source">Stream' = (path, true, data(FS, path))
</pre></div></div>

<div class="section">
<h4><a name="Persisting_data"></a>Persisting data</h4>
<p>When the stream writes data back to its store, be it in any supported flush operation, in the <tt>close()</tt> operation, or at any other time the stream chooses to do so, the contents of the file are replaced with the current buffer</p>

<div>
<div>
<pre class="source">Stream' = (path, true, buffer)
FS' = FS where data(FS', path) == buffer
</pre></div></div>

<p>After a call to <tt>close()</tt>, the stream is closed for all operations other than <tt>close()</tt>; they MAY fail with <tt>IOException</tt> or <tt>RuntimeException</tt>.</p>

<div>
<div>
<pre class="source">Stream' =  (path, false, [])
</pre></div></div>

<p>The <tt>close()</tt> operation must be idempotent with the sole attempt to write the data made in the first invocation.</p>
<ol style="list-style-type: decimal">

<li>If <tt>close()</tt> succeeds, subsequent calls are no-ops.</li>
<li>If <tt>close()</tt> fails, again, subsequent calls are no-ops. They MAY rethrow the previous exception, but they MUST NOT retry the write.</li>
</ol><!--  ============================================================= -->
<!--  CLASS: FSDataOutputStream -->
<!--  ============================================================= -->

<h1>Class <tt>FSDataOutputStream</tt></h1>

<div>
<div>
<pre class="source">public class FSDataOutputStream
  extends DataOutputStream
  implements Syncable, CanSetDropBehind, StreamCapabilities {
 // ...
}
</pre></div></div>

<p>The <tt>FileSystem.create()</tt>, <tt>FileSystem.append()</tt> and <tt>FSDataOutputStreamBuilder.build()</tt> calls return an instance of a class <tt>FSDataOutputStream</tt>, a subclass of <tt>java.io.OutputStream</tt>.</p>
<p>The base class wraps an <tt>OutputStream</tt> instance, one which may implement <tt>Streamable</tt>, <tt>CanSetDropBehind</tt> and <tt>StreamCapabilities</tt>.</p>
<p>This document covers the requirements of such implementations.</p>
<p>HDFS&#x2019;s <tt>FileSystem</tt> implementation, <tt>DistributedFileSystem</tt>, returns an instance of <tt>HdfsDataOutputStream</tt>. This implementation has at least two behaviors which are not explicitly declared by the base Java implmentation</p>
<ol style="list-style-type: decimal">

<li>

<p>Writes are synchronized: more than one thread can write to the same output stream. This is a use pattern which HBase relies on.</p>
</li>
<li>

<p><tt>OutputStream.flush()</tt> is a no-op when the file is closed. Apache Druid has made such a call on this in the past <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-14346">HADOOP-14346</a>.</p>
</li>
</ol>
<p>As the HDFS implementation is considered the de-facto specification of the FileSystem APIs, the fact that <tt>write()</tt> is thread-safe is significant.</p>
<p>For compatibility, not only must other FS clients be thread-safe, but new HDFS featues, such as encryption and Erasure Coding must also implement consistent behavior with the core HDFS output stream.</p>
<p>Put differently:</p>
<p><i>It isn&#x2019;t enough for Output Streams to implement the core semantics of <tt>java.io.OutputStream</tt>: they need to implement the extra semantics of <tt>HdfsDataOutputStream</tt>. Failure to do so must be considered regressions</i></p>
<p>The concurrent <tt>write()</tt> call is the most significant tightening of the Java specification.</p></div></div></div>
<div class="section">
<h2><a name="Class_java.io.OutputStream"></a>Class <tt>java.io.OutputStream</tt></h2>
<p>A Java <tt>OutputStream</tt> allows applications to write a sequence of bytes to a destination. In a Hadoop filesystem, that destination is the data under a path in the filesystem.</p>

<div>
<div>
<pre class="source">public abstract class OutputStream implements Closeable, Flushable {
  public abstract void write(int b) throws IOException;
  public void write(byte b[]) throws IOException;
  public void write(byte b[], int off, int len) throws IOException;
  public void flush() throws IOException ;
  public void close() throws IOException;
}
</pre></div></div>

<div class="section">
<h3><a name="write.28Stream.2C_data.29"></a><a name="writedata:_int"></a><tt>write(Stream, data)</tt></h3>
<p>Writes a byte of data to the stream.</p>
<div class="section">
<h4><a name="Preconditions"></a>Preconditions</h4>

<div>
<div>
<pre class="source">Stream.open else raise ClosedChannelException, PathIOException, IOException
</pre></div></div>

<p>The exception <tt>java.nio.channels.ClosedChannelExceptionn</tt> is raised in the HDFS output streams when trying to write to a closed file. Ths exception does not include the destination path; and <tt>Exception.getMessage()</tt> is <tt>null</tt>. It is therefore of limited value in stack traces. Implementors may wish to raise exceptions with more detail, such as a <tt>PathIOException</tt>.</p></div>
<div class="section">
<h4><a name="Postconditions"></a>Postconditions</h4>
<p>The buffer has the lower 8 bits of the data argument appended to it.</p>

<div>
<div>
<pre class="source">Stream'.buffer = Stream.buffer + [data &amp; 0xff]
</pre></div></div>

<p>There may be an explicit limit on the size of cached data, or an implicit limit based by the available capacity of the destination filesystem. When a limit is reached, <tt>write()</tt> SHOULD fail with an <tt>IOException</tt>.</p></div></div>
<div class="section">
<h3><a name="write.28Stream.2C_byte.5B.5D_data.2C_int_offset.2C_int_len.29"></a><a name="writebufferoffsetlen"></a><tt>write(Stream, byte[] data, int offset, int len)</tt></h3>
<div class="section">
<h4><a name="Preconditions"></a>Preconditions</h4>
<p>The preconditions are all defined in <tt>OutputStream.write()</tt></p>

<div>
<div>
<pre class="source">Stream.open else raise ClosedChannelException, PathIOException, IOException
data != null else raise NullPointerException
offset &gt;= 0 else raise IndexOutOfBoundsException
len &gt;= 0 else raise IndexOutOfBoundsException
offset &lt; data.length else raise IndexOutOfBoundsException
offset + len &lt; data.length else raise IndexOutOfBoundsException
</pre></div></div>

<p>There may be an explicit limit on the size of cached data, or an implicit limit based by the available capacity of the destination filesystem. When a limit is reached, <tt>write()</tt> SHOULD fail with an <tt>IOException</tt>.</p>
<p>After the operation has returned, the buffer may be re-used. The outcome of updates to the buffer while the <tt>write()</tt> operation is in progress is undefined.</p></div>
<div class="section">
<h4><a name="Postconditions"></a>Postconditions</h4>

<div>
<div>
<pre class="source">Stream'.buffer = Stream.buffer + data[offset...(offset + len)]
</pre></div></div>
</div></div>
<div class="section">
<h3><a name="write.28byte.5B.5D_data.29"></a><a name="writebuffer"></a><tt>write(byte[] data)</tt></h3>
<p>This is defined as the equivalent of:</p>

<div>
<div>
<pre class="source">write(data, 0, data.length)
</pre></div></div>
</div>
<div class="section">
<h3><a name="flush.28.29"></a><a name="flush"></a><tt>flush()</tt></h3>
<p>Requests that the data is flushed. The specification of <tt>ObjectStream.flush()</tt> declares that this SHOULD write data to the &#x201c;intended destination&#x201d;.</p>
<p>It explicitly precludes any guarantees about durability.</p>
<p>For that reason, this document doesn&#x2019;t provide any normative specifications of behaviour.</p>
<div class="section">
<h4><a name="Preconditions"></a>Preconditions</h4>

<div>
<div>
<pre class="source">Stream.open else raise IOException
</pre></div></div>
</div>
<div class="section">
<h4><a name="Postconditions"></a>Postconditions</h4>
<p>None.</p>
<p>If the implementation chooses to implement a stream-flushing operation, the data may be saved to the file system such that it becomes visible to others&quot;</p>

<div>
<div>
<pre class="source">FS' = FS where data(FS, path) == buffer
</pre></div></div>

<p>Some applications have been known to call <tt>flush()</tt> on a closed stream on the assumption that it is harmless. Implementations MAY choose to support this behaviour.</p></div></div>
<div class="section">
<h3><a name="close.28.29"></a><a name="close"></a><tt>close()</tt></h3>
<p>The <tt>close()</tt> operation saves all data to the filesystem and releases any resources used for writing data.</p>
<p>The <tt>close()</tt> call is expected to block until the write has completed (as with <tt>Syncable.hflush()</tt>), possibly until it has been written to durable storage (as HDFS does).</p>
<p>After <tt>close()</tt> completes, the data in a file MUST be visible and consistent with the data most recently written. The metadata of the file MUST be consistent with the data and the write history itself (i.e. any modification time fields updated).</p>
<p>After <tt>close()</tt> is invoked, all subsequent <tt>write()</tt> calls on the stream MUST fail with an <tt>IOException</tt>.</p>
<p>Any locking/leaseholding mechanism is also required to release its lock/lease.</p>

<div>
<div>
<pre class="source">Stream'.open = false
FS' = FS where data(FS, path) == buffer
</pre></div></div>

<p>The <tt>close()</tt> call MAY fail during its operation.</p>
<ol style="list-style-type: decimal">

<li>Callers of the API MUST expect for some calls to fail and SHOULD code appropriately. Catching and swallowing exceptions, while common, is not always the ideal solution.</li>
<li>Even after a failure, <tt>close()</tt> MUST place the stream into a closed state. Follow-on calls to <tt>close()</tt> are ignored, and calls to other methods rejected. That is: caller&#x2019;s cannot be expected to call <tt>close()</tt> repeatedly until it succeeds.</li>
<li>The duration of the <tt>call()</tt> operation is undefined. Operations which rely on acknowledgements from remote systems to meet the persistence guarantees implicitly have to await these acknowledgements. Some Object Store output streams upload the entire data file in the <tt>close()</tt> operation. This can take a large amount of time. The fact that many user applications assume that <tt>close()</tt> is both fast and does not fail means that this behavior is dangerous.</li>
</ol>
<p>Recommendations for safe use by callers</p>
<ul>

<li>Do plan for exceptions being raised, either in catching and logging or by throwing the exception further up. Catching and silently swallowing exceptions may hide serious problems.</li>
<li>Heartbeat operations SHOULD take place on a separate thread, so that a long delay in <tt>close()</tt> does not block the thread so long that the heartbeat times out.</li>
</ul></div></div>
<div class="section">
<h2><a name="Interface_StreamCapabilities"></a>Interface <tt>StreamCapabilities</tt></h2>

<div>
<div>
<pre class="source">@InterfaceAudience.Public
@InterfaceStability.Evolving
</pre></div></div>

<p>The <tt>StreamCapabilities</tt> interface exists to allow callers to dynamically determine the behavior of a stream.</p>
<p>The reference Implementation of this interface is <tt>org.apache.hadoop.hdfs.DFSOutputStream</tt></p>

<div>
<div>
<pre class="source">public boolean hasCapability(String capability) {
  if (capability.equalsIgnoreCase(HSYNC.getValue()) ||
      capability.equalsIgnoreCase((HFLUSH.getValue()))) {
    return true;
  }
  return false;
}
</pre></div></div>

<p>Where <tt>HSYNC</tt> and <tt>HFLUSH</tt> are items in the enumeration <tt>org.apache.hadoop.fs.StreamCapabilities.StreamCapability</tt>.</p></div>
<div class="section">
<h2><a name="org.apache.hadoop.fs.Syncable"></a><a name="syncable"></a><tt>org.apache.hadoop.fs.Syncable</tt></h2>

<div>
<div>
<pre class="source">@InterfaceAudience.Public
@InterfaceStability.Evolving
public interface Syncable {
  /**
   * @deprecated As of HADOOP 0.21.0, replaced by hflush
   * @see #hflush()
   */
  @Deprecated void sync() throws IOException;

  /** Flush out the data in client's user buffer. After the return of
   * this call, new readers will see the data.
   * @throws IOException if any error occurs
   */
  void hflush() throws IOException;

  /** Similar to posix fsync, flush out the data in client's user buffer
   * all the way to the disk device (but the disk may have it in its cache).
   * @throws IOException if error occurs
   */
  void hsync() throws IOException;
}
</pre></div></div>

<p>The purpose of <tt>Syncable</tt> interface is to provide guarantees that data is written to a filesystem for both visibility and durability.</p>
<p><i>SYNC-1</i>: An <tt>OutputStream</tt> which implements <tt>Syncable</tt> is making an explicit declaration of an that it can meet those guarantees. meet those guarantees.</p>
<p><i>SYNC-2</i>: The interface MUST NOT be declared as implemented by an <tt>OutputStream</tt> unless those guarantees can be met.</p>
<p>The <tt>Syncable</tt> interface has been implemented by other classes than subclasses of <tt>OutputStream</tt>, such as <tt>org.apache.hadoop.io.SequenceFile.Writer</tt>.</p>
<p><i>SYNC-3</i> The fact that a class implements <tt>Syncable</tt> does not guarantee that <tt>extends OutputStream</tt> holds.</p>
<p>That is, for any class <tt>C</tt>: <tt>(C instanceof Syncable)</tt> does not imply <tt>(C instanceof OutputStream)</tt></p>
<p>This specification only covers the required behavior of <tt>OutputStream</tt> subclasses which implement <tt>Syncable</tt>.</p>
<p><i>SYNC-4:</i> The return value of <tt>FileSystem.create(Path)</tt> is an instance of <tt>FSDataOutputStream</tt>.</p>
<p><i>SYNC-5:</i> <tt>FSDataOutputStream implements Syncable</tt></p>
<p>SYNC-5 and SYNC-1 imply that all output streams which can be created with <tt>FileSystem.create(Path)</tt> must support the semantics of <tt>Syncable</tt>. Any inspection of the Hadoop codebase will make clear that this is demonstrably not true, therefore these declarations SYNC-1 and SYNC-2 are demonstrably false.</p>
<p>Put differently: <i>callers cannot rely on the presence of the interface as evidence that the semantics of <tt>Syncable</tt> are supported</i>. Instead they should be dynamically probed for using the <tt>StreamCapabilities</tt> interface, where available.</p>
<div class="section">
<h3><a name="Syncable.hflush.28.29"></a><a name="Syncable.hflush"></a><tt>Syncable.hflush()</tt></h3>
<p>Flush out the data in client&#x2019;s user buffer. After the return of this call, new readers will see the data. The <tt>hflush()</tt> operation does contain guarantees as to the durability of the data. only is visibility. Thus implementations may cache the written data in memory &#x2014;visible to all, but not yet persisted.</p>
<div class="section">
<h4><a name="Preconditions"></a>Preconditions</h4>

<div>
<div>
<pre class="source">hasCapability(Stream. &quot;hflush&quot;)
Stream.open else raise IOException
</pre></div></div>
</div>
<div class="section">
<h4><a name="Postconditions"></a>Postconditions</h4>

<div>
<div>
<pre class="source">FS' = FS where data(path) == cache
</pre></div></div>

<p>After the call returns, the data MUST be visible to all new callers of <tt>FileSystem.open</tt>.</p>
<p>There is no requirement or guarantee that clients with an existing <tt>DataInputStream</tt> created by a call to <tt>(FS, path)</tt> will see the updated data, nor is there a guarantee that they <i>will not</i> in a current or subsequent read.</p>
<p>Implementation note: as a correct <tt>hsync()</tt> implementation must also offer all the semantics of an <tt>hflush()</tt> call, implementations of <tt>hflush()</tt> may just invoke <tt>hsync()</tt>:</p>

<div>
<div>
<pre class="source">public void hflush() throws IOException {
  hsync();
}
</pre></div></div>
</div></div>
<div class="section">
<h3><a name="Syncable.hsync.28.29"></a><a name="Syncable.hsync"></a><tt>Syncable.hsync()</tt></h3>
<p>Similar to POSIX <tt>fsync</tt>, this call saves the data in client&#x2019;s user buffer all the way to the disk device (but the disk may have it in its cache).</p>
<p>That is: it is a requirement for the underlying FS To save all the data to the disk hardware itself, where it is expected to be durable.</p>
<div class="section">
<h4><a name="Preconditions"></a>Preconditions</h4>

<div>
<div>
<pre class="source">hasCapability(Stream, &quot;hsync&quot;)
Stream.open else raise IOException
</pre></div></div>
</div>
<div class="section">
<h4><a name="Postconditions"></a>Postconditions</h4>

<div>
<div>
<pre class="source">FS' = FS where data(path) == buffer
</pre></div></div>

<p>The reference implementation, <tt>DFSOutputStream</tt> will block until an acknowledgement is received from the datanodes: That is, all hosts in the replica write chain have successfully written the file.</p>
<p>That means that the expectation callers may have is that the return of the method call contains visibility and durability guarantees which other implementations must maintain.</p>
<p>Note, however, that the reference <tt>DFSOutputStream.hsync()</tt> call only actually syncs/ <i>the current block</i>. If there have been a series of writes since the last sync, such that a block boundary has been crossed. The <tt>hsync()</tt> call claims only to write the most recent.</p>
<p>From the javadocs of <tt>DFSOutputStream.hsync(EnumSet&lt;SyncFlag&gt; syncFlags)</tt></p>
<blockquote>

<p>Note that only the current block is flushed to the disk device. To guarantee durable sync across block boundaries the stream should be created with {@link CreateFlag#SYNC_BLOCK}.</p>
</blockquote>
<p>In virtual machines, the notion of &#x201c;disk hardware&#x201d; is really that of another software abstraction: there are guarantees.</p></div></div></div>
<div class="section">
<h2><a name="interface_CanSetDropBehind"></a><tt>interface CanSetDropBehind</tt></h2>

<div>
<div>
<pre class="source">@InterfaceAudience.Public
@InterfaceStability.Evolving
public interface CanSetDropBehind {
  /**
   * Configure whether the stream should drop the cache.
   *
   * @param dropCache     Whether to drop the cache.  null means to use the
   *                      default value.
   * @throws IOException  If there was an error changing the dropBehind
   *                      setting.
   *         UnsupportedOperationException  If this stream doesn't support
   *                                        setting the drop-behind.
   */
  void setDropBehind(Boolean dropCache)
      throws IOException, UnsupportedOperationException;
}
</pre></div></div>

<p>This interface allows callers to change policies used inside HDFS. They are currently unimplemented by any stream other than those in HDFS.</p></div>
<div class="section">
<h2><a name="Durability.2C_Concurrency.2C_Consistency_and_Visibility_of_stream_output."></a>Durability, Concurrency, Consistency and Visibility of stream output.</h2>
<p>These are the aspects of the system behaviour which are not directly covered in this (very simplistic) Filesystem model, but which are visible in production.</p></div>
<div class="section">
<h2><a name="Durability"></a>Durability</h2>
<ol style="list-style-type: decimal">

<li><tt>OutputStream.write()</tt> MAY persist the data, synchronously or asynchronously</li>
<li><tt>OutputStream.flush()</tt> flushes data to the destination. There are no strict persistence requirements.</li>
<li><tt>Syncable.hflush()</tt> synchronously sends all local data to the destination filesystem. After returning to the caller, the data MUST be visible to other readers, it MAY be durable. That is: it does not have to be persisted, merely guaranteed to be consistently visible to all clients attempting to open a new stream reading data at the path.</li>
<li><tt>Syncable.hsync()</tt> MUST flush the data and persist data to the underlying durable storage.</li>
<li><tt>close()</tt> The first call to <tt>close()</tt> MUST flush out all remaining data in the buffers, and persist it.</li>
</ol></div>
<div class="section">
<h2><a name="Concurrency"></a>Concurrency</h2>
<ol style="list-style-type: decimal">

<li>

<p>The outcome of more than one process writing to the same file is undefined.</p>
</li>
<li>

<p>An input stream opened to read a file <i>before the file was opened for writing</i> MAY fetch data updated by writes to an OutputStream. Because of buffering and caching, this is not a requirement &#x2014;and if a input stream does pick up updated data, the point at which the updated data is read is undefined. This surfaces in object stores where a <tt>seek()</tt> call which closes and re-opens the connection may pick up updated data, while forward stream reads do not. Similarly, in block-oriented filesystems, the data may be cached a block at a time &#x2014;and changes only picked up when a different block is read.</p>
</li>
<li>

<p>A Filesystem MAY allow the destination path to be manipulated while a stream is writing to it &#x2014;for example, <tt>rename()</tt> of the path or a parent; <tt>delete()</tt> of a path or parent. In such a case, the outcome of future write operations on the output stream is undefined. Some filesystems MAY implement locking to prevent conflict. However, this tends to be rare on distributed filesystems, for reasons well known in the literature.</p>
</li>
<li>

<p>The Java API specification of <tt>java.io.OutputStream</tt> does not require an instance of the class to be thread safe. However, <tt>org.apache.hadoop.hdfs.DFSOutputStream</tt> has a stronger thread safety model (possibly unintentionally). This fact is relied upon in Apache HBase, as discovered in HADOOP-11708. Implementations SHOULD be thread safe. <i>Note</i>: even the <tt>DFSOutputStream</tt> synchronization model permits the output stream to <tt>close()</tt>&#x2019;d while awaiting an acknowledgement from datanode or namenode writes in an <tt>hsync()</tt> operation.</p>
</li>
</ol></div>
<div class="section">
<h2><a name="Consistency_and_Visibility"></a>Consistency and Visibility</h2>
<p>There is no requirement for the data to be immediately visible to other applications &#x2014;not until a specific call to flush buffers or persist it to the underlying storage medium are made.</p>
<p>If an output stream is created with <tt>FileSystem.create(path, overwrite==true)</tt> and there is an existing file at the path, that is <tt>exists(FS, path)</tt> holds, then, the existing data is immediately unavailable; the data at the end of the path MUST consist of an empty byte sequence <tt>[]</tt>, with consistent metadata.</p>

<div>
<div>
<pre class="source">exists(FS, path)
(Stream', FS') = create(FS, path)
exists(FS', path)
getFileStatus(FS', path).getLen() = 0
</pre></div></div>

<p>The metadata of a file (<tt>length(FS, path)</tt> in particular) SHOULD be consistent with the contents of the file after <tt>flush()</tt> and <tt>sync()</tt>.</p>

<div>
<div>
<pre class="source">(Stream', FS') = create(FS, path)
(Stream'', FS'') = write(Stream', data)
(Stream''', FS''') hsync(Stream'')
exists(FS''', path)
getFileStatus(FS''', path).getLen() = len(data)
</pre></div></div>

<p>HDFS does not do this except when the write crosses a block boundary; to do otherwise would overload the Namenode. As a result, while a file is being written <tt>length(Filesystem, Path)</tt> MAY be less than the length of <tt>data(Filesystem, Path)</tt>.</p>
<p>The metadata MUST be consistent with the contents of a file after the <tt>close()</tt> operation.</p>
<p>After the contents of an output stream have been persisted (<tt>hflush()/hsync()</tt>) all new <tt>open(FS, Path)</tt> operations MUST return the updated data.</p>
<p>After <tt>close()</tt> has been invoked on an output stream, a call to <tt>getFileStatus(path)</tt> MUST return the final metadata of the written file, including length and modification time. The metadata of the file returned in any of the FileSystem <tt>list</tt> operations MUST be consistent with this metadata.</p>
<p>The value of <tt>getFileStatus(path).getModificationTime()</tt> is not defined while a stream is being written to. The timestamp MAY be updated while a file is being written, especially after a <tt>Syncable.hsync()</tt> call. The timestamps MUST be updated after the file is closed to that of a clock value observed by the server during the <tt>close()</tt> call. It is <i>likely</i> to be in the time and time zone of the filesystem, rather than that of the client.</p>
<p>Formally, if a <tt>close()</tt> operation triggers an interaction with a server which starts at server-side time <tt>t1</tt> and completes at time <tt>t2</tt> with a successfully written file, then the last modification time SHOULD be a time <tt>t</tt> where <tt>t1 &lt;= t &lt;= t2</tt></p></div>
<div class="section">
<h2><a name="Issues_with_the_Hadoop_Output_Stream_model."></a>Issues with the Hadoop Output Stream model.</h2>
<p>There are some known issues with the output stream model as offered by Hadoop, specifically about the guarantees about when data is written and persisted &#x2014;and when the metadata is synchronized. These are where implementation aspects of HDFS and the &#x201c;Local&#x201d; filesystem do not the simple model of the filesystem used in this specification.</p>
<div class="section">
<h3><a name="HDFS"></a>HDFS</h3>
<p>That HDFS file metadata often lags the content of a file being written to is not something everyone expects, nor convenient for any program trying pick up updated data in a file being written. Most visible is the length of a file returned in the various <tt>list</tt> commands and <tt>getFileStatus</tt> &#x2014;this is often out of data.</p>
<p>As HDFS only supports file growth in its output operations, this means that the size of the file as listed in the metadata may be less than or equal to the number of available bytes &#x2014;but never larger. This is a guarantee which is also held</p>
<p>One Algorithm to determine whether a file in HDFS is updated is:</p>
<ol style="list-style-type: decimal">

<li>Remember the last read position <tt>pos</tt> in the file, using <tt>0</tt> if this is the initial read.</li>
<li>Use <tt>getFileStatus(FS, Path)</tt> to query the updated length of the file as recorded in the metadata.</li>
<li>If <tt>Status.length &amp;gt pos</tt>, the file has grown.</li>
<li>If the number has not changed, then
<ol style="list-style-type: decimal">

<li>reopen the file.</li>
<li><tt>seek(pos)</tt> to that location</li>
<li>If <tt>read() != -1</tt>, there is new data.</li>
</ol>
</li>
</ol>
<p>This algorithm works for filesystems which are consistent with metadata and data, as well as HDFS. What is important to know is that, in HDFS <tt>getFileStatus(FS, path).getLen()==0</tt> does not imply that <tt>data(FS, path)</tt> is empty.</p></div>
<div class="section">
<h3><a name="Local_Filesystem.2C_file:"></a>Local Filesystem, <tt>file:</tt></h3>
<p><tt>LocalFileSystem</tt>, <tt>file:</tt>, (or any other <tt>FileSystem</tt> implementation based on <tt>ChecksumFileSystem</tt>) has a different issue. If an output stream is obtained from <tt>create()</tt> and <tt>FileSystem.setWriteChecksum(false)</tt> has <i>not</i> been called on the filesystem, then the FS only flushes as much local data as can be written to full checksummed blocks of data.</p>
<p>That is, the flush operations are not guaranteed to write all the pending data until the file is finally closed.</p>
<p>That is, <tt>sync()</tt>, <tt>hsync()</tt> and <tt>hflush</tt> may not persist all data written to the stream.</p>
<p>For anyone thinking &#x201c;this is a violation of this specification&#x201d; &#x2014;they are correct. The local filesystem is intended for testing, rather than production use.</p></div>
<div class="section">
<h3><a name="Checksummed_output_streams"></a>Checksummed output streams</h3>
<p>Because  <tt>org.apache.hadoop.fs.FSOutputSummer</tt> and <tt>org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSOutputSummer</tt> implement the underlying checksummed output stream used by HDFS and other filesystems, it provides some of the core semantics of the output stream behavior.</p>
<ol style="list-style-type: decimal">

<li>The <tt>close()</tt> call is unsynchronized, re-entrant and may attempt to close the stream more than once.</li>
<li>It is possible to call <tt>write(int)</tt> on a closed stream (but not <tt>write(byte[], int, int)</tt>).</li>
<li>It is possible to call <tt>flush()</tt> on a closed stream.</li>
</ol>
<p>Behaviors 1 &amp; 2 really have to be considered bugs to fix, albeit with care.</p></div>
<div class="section">
<h3><a name="Object_Stores"></a>Object Stores</h3>
<p>Object store streams tend to buffer the entire stream&#x2019;s output until the final <tt>close()</tt> operation triggers a single <tt>PUT</tt> of the data and materialization of the final output.</p>
<p>This significantly change&#x2019;s their behaviour compared to that of POSIX filesystems and that specified in this document.</p>
<div class="section">
<h4><a name="Visibility_of_newly_created_files"></a>Visibility of newly created files</h4>
<p>There is no guarantee that any file will be visible at the path of an output stream after the output stream is created .</p>
<p>That is: while <tt>create(FS, path, boolean)</tt> returns a new stream</p>

<div>
<div>
<pre class="source">Stream' = (path, true, [])
</pre></div></div>

<p>The other postcondition of the operation, <tt>data(FS', path) == []</tt> may not hold,</p>
<ol style="list-style-type: decimal">

<li><tt>exists(FS, p)</tt> MAY return false.</li>
<li>If a file was created with <tt>overwrite = True</tt>, the existing data my still be visible: <tt>data(FS', path) = data(FS, path)</tt>.</li>
<li>

<p>The check for existing data in a <tt>create()</tt> call with <tt>overwrite=False</tt>, may take place in the <tt>create()</tt> call itself, in the <tt>close()</tt> call prior to/during the write, or at some point in between. Expect in the special case that the object store supports an atomic PUT operation, the check for existence of existing data and the subsequent creation of data at the path contains a race condition: other clients may create data at the path between the existence check and the subsequent qrite.</p>
</li>
<li>

<p>Calls to <tt>create(FS, Path, overwrite=false)</tt> may succeed, returning a new <tt>OutputStream</tt>, even while another stream is open and writing to the destination path.</p>
</li>
</ol>
<p>This allows for the following sequence of operations, which would raise an exception in the second <tt>open()</tt> call if invoked against HDFS:</p>

<div>
<div>
<pre class="source">Stream1 = open(FS, path, false)
sleep(200)
Stream2 = open(FS, path, false)
Stream.write('a')
Stream1.close()
Stream2.close()
</pre></div></div>

<p>For anyone wondering why the clients create a 0-byte file in the create call, it would cause problems after <tt>close()</tt> &#x2014;the marker file could get returned in <tt>open()</tt> calls instead of the final data.</p></div>
<div class="section">
<h4><a name="Visibility_of_the_output_of_a_stream_after_close.28.29"></a>Visibility of the output of a stream after <tt>close()</tt></h4>
<p>One guarantee which Object Stores SHOULD make is the same as those of POSIX filesystems: After a stream <tt>close()</tt> call returns, the data MUST be persisted durably and visible to all callers. Unfortunately, even that guarantee is not always met:</p>
<ol style="list-style-type: decimal">

<li>

<p>Existing data on a path MAY be visible for an indeterminate period of time.</p>
</li>
<li>

<p>If the store has any form of create inconsistency or buffering of negative existence probes, then even after the stream&#x2019;s <tt>close()</tt> operation has returned, <tt>getFileStatus(FS, path)</tt> and <tt>open(FS, path)</tt> may fail with a <tt>FileNotFoundException</tt>.</p>
</li>
</ol>
<p>In their favour, the atomicity of the store&#x2019;s PUT operations do offer their own guarantee: an newly created object is either absent or all of its data is present: the act of instantiatng the object, while potentially exhibiting create inconsistency, is atomic. Applications may be able to use that fact to their advantage.</p></div>
<div class="section">
<h4><a name="Other_issues"></a>Other issues</h4>
<p>The <tt>Syncable</tt> interfaces and methods are rarely implemented. Use <tt>StreamCapabilities</tt> to determine their availability (Azure&#x2019;s <tt>PageBlobOutputStream</tt> is the sole &#x201c;syncable&#x201d; object store output stream).</p></div></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2008-2019
              Apache Software Foundation
            
                          - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
